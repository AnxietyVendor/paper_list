% Autogenerated translation of README.md by Texpad
% To stop this file being overwritten during the typeset process, please move or remove this header

\documentclass[12pt]{book}
\usepackage{graphicx}
\usepackage{fontspec}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,left=.5in,right=.5in,top=.3in,bottom=0.3in]{geometry}
\setlength\parindent{0pt}
\setlength{\parskip}{\baselineskip}
\setmainfont{Helvetica Neue}
\usepackage{hyperref}
\pagestyle{plain}
\begin{document}

$\vert$ Datasets       $\vert$         R-1          $\vert$         R-2          $\vert$         R-L          $\vert$                            Methods                            $\vert$
$\vert$ :--------------: $\vert$ :------------------: $\vert$ :------------------: $\vert$ :------------------: $\vert$ :----------------------------------------------------------: $\vert$
$\vert$ CNN/DM         $\vert$ 43.77\texttt{$<$br /$>$}\textbf{44.41} $\vert$ 20.85\texttt{$<$br /$>$}\textbf{20.86} $\vert$ \textbf{40.67}\texttt{$<$br /$>$}40.55 $\vert$ \href{./paper/2020.acl-main.451.pdf}{DISCOBERT W.GR\&GC}\texttt{$<$br /$>$}\href{./paper/2004.08795.pdf}{MATCHSUM (ROBERTa-base)} $\vert$
$\vert$ Gigaword       $\vert$        40.89         $\vert$        19.11         $\vert$        37.60         $\vert$              \href{./paper/1911.10390.pdf}{best-abs}              $\vert$
$\vert$ DUC2004        $\vert$        31.00         $\vert$        11.11         $\vert$        26.94         $\vert$              \href{./paper/AAAI-JinH.7203.pdf}{SemSUM}             $\vert$
$\vert$ NewSroom       $\vert$        45.93         $\vert$        24.14         $\vert$        42.51         $\vert$              \href{./paper/1911.10390.pdf}{best-abs}              $\vert$
$\vert$ XSUM           $\vert$        24.86         $\vert$         4.58         $\vert$        18.31         $\vert$        \href{./paper/2004.08795.pdf}{MATCHSUM (Sel = 1,2)}        $\vert$
$\vert$ New York Times $\vert$        50.00         $\vert$        30.38         $\vert$        42.70         $\vert$      \href{./paper/2020.acl-main.451.pdf}{DISCOBERT W.GR\&GC}      $\vert$
$\vert$ MSR-ATC        $\vert$        33.82         $\vert$        17.08         $\vert$        30.62         $\vert$             \href{./paper/AAAI-JinH.7203.pdf}{SemSUM}             $\vert$
$\vert$ Reddit         $\vert$        25.09         $\vert$         6.17         $\vert$        20.13         $\vert$        \href{./paper/2004.08795.pdf}{MATCHSUM (Sel = 1,2)}        $\vert$

\section*{2020}

\begin{quote}
\subsubsection*{EMNLP 2020: \href{./paper/2020-Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward.pdf}{Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward}}

\begin{quote}
Author: Luyang Huang, Lingfei Wu, Lu Wang1

Dataset: CNN/Daily Mail,New York Times
Score: \\$\vert$ Model $\vert$  CNN/DM\texttt{$<$br$>$} R-1        R-2        R-L$\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ASGARD-SEG +R\texttt{$<$sub$>$}rouge\texttt{$<$/sub$>$}+R\texttt{$<$sub$>$}close\texttt{$<$/sub$>$}$\vert$   43.81   20.22   40.37 $\vert$
$\vert$ Model $\vert$ NYT\texttt{$<$br$>$} R-1        R-2        R-L$\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ASGARD-SEG +R\texttt{$<$sub$>$}rouge\texttt{$<$/sub$>$}+R\texttt{$<$sub$>$}close\texttt{$<$/sub$>$}$\vert$   51.29   34.97   48.26 $\vert$
\end{quote}

\subsubsection*{EMNLP 2020: \href{./paper/2020-Better Highlighting Creating Sub-Sentence Summary Highlights.pdf}{Better Highlighting: Creating Sub-Sentence Summary Highlights}}

\begin{quote}
Author: Sangwoo Cho, Kaiqiang Song, Chen Li,Dong Yu, Hassan Foroosh, Fei Liu

Dataset: DUC-04 
Score: \\$\vert$ Model $\vert$  R-1        R-2        R-SU4$\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ HL-TreeSegs  $\vert$   39.18   10.30   14.37 $\vert$
$\vert$ HL-XLNetSegs  $\vert$   39.26    10.70   14.47$\vert$
\end{quote}

\subsubsection*{EMNLP 2020: \href{./paper/2020-Multi-Fact Correction in Abstractive Text Summarization.pdf}{Multi-Fact Correction in Abstractive Text Summarization}}

\begin{quote}
Author: Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung,  Jingjing Liu

Dataset: CNN/DailyMail
Score: \\$\vert$ Model $\vert$  QGQA  $\vert$  FactCC sent  $\vert$  R-1        R-2        R-L$\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$:---: $\vert$:---: $\vert$
$\vert$ QA-Span  $\vert$   75.94   $\vert$   80.97  $\vert$   41.75   19.27   38.81 $\vert$
$\vert$ Auto-regressive $\vert$   75.19    $\vert$   79.89  $\vert$   41.68  19.16   38.74 $\vert$
\end{quote}

\subsubsection*{EMNLP 2020: \href{./paper/2020-emnlp-Evaluating the Factual Consistency of Abstractive Text Summarization.pdf}{Evaluating the Factual Consistency of Abstractive Text Summarization}}

\begin{quote}
Author: Wojciech Krys ́cin ́ski, Bryan McCann, Caiming Xiong, Richard Socher

Dataset: CNN/DailyMail
Score: \\$\vert$ Model $\vert$  Incorrect  $\vert$  \$Delta\$  $\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ FactCC  $\vert$ 30.0\% $\vert$  -20.0 $\vert$
\end{quote}

\subsubsection*{EMNLP 2020: \href{./paper/2020-PALM Pre-training an Autoencoding&Autoregressive Language Modelfor Context-conditioned Generation.pdf}{PALM Pre-training an Autoencoding\&Autoregressive Language Modelfor Context-conditioned Generation}}

\begin{quote}
Author: Bin Bi, Chenliang Li, Chen Wu, Ming Yan,Wei Wang, Songfang Huang, Fei Huang, Luo Si

Dataset: CNN/DailyMail,Gigaword
Score: 
 $\vert$ Methods                 $\vert$ CNN/DailyMail\texttt{$<$br$>$}R-1        R-2        R-L $\vert$ Gigaword\texttt{$<$br$>$}R-1        R-2        R-L $\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ PALM                    $\vert$          42.71   19.97     39.71          $\vert$        38.75   19.79   35.98        $\vert$
$\vert$ PALM(LARGE)                    $\vert$         44.30   21.12     41.41          $\vert$        39.45   20.37   36.75        $\vert$
\end{quote}

\subsubsection*{EMNLP 2020: \href{./paper/2020-Pre-training for Abstractive Document Summarization byReinstating Source Text.pdf}{Pre-training for Abstractive Document Summarization by Reinstating Source Text}}

\begin{quote}
Author: Yanyan Zou, Xingxing Zhang, Wei Lu,Furu Wei, Ming Zhou

Dataset: CNN/DailyMail
Score: \\$\vert$ Model $\vert$  R-1  $\vert$  R-2  $\vert$  R-L  $\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ STEP  $\vert$ 44.03 $\vert$ 21.13 $\vert$ 41.20 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/2020-Keywords-Guided Abstractive Sentence Summarization.pdf}{Keywords-Guided Abstractive Sentence Summarization}}

\begin{quote}
Author: Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong, Xiaodong He

Dataset: Gigaword
Score:  

$\vert$ Model $\vert$  R-1        R-2        R-L$\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ Co- Selective  Hier+DualPG  $\vert$  47.14   25.06  44.39 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-LiH.902.pdf}{Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products}}

\begin{quote}
Author: Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, Bowen Zhou

Dataset: CEPSUM (A new dataset from JD.com)
Score:  

$\vert$ Methods                 $\vert$ Home Appliances\texttt{$<$br$>$}R-1        R-2        R-L $\vert$ Clothing\texttt{$<$br$>$}R-1        R-2        R-L $\vert$ Cases\&Bags\texttt{$<$br$>$}R-1        R-2        R-L $\vert$
$\vert$ :---------------------- $\vert$ :------------------------------------------: $\vert$ :-----------------------------------: $\vert$ :-------------------------------------: $\vert$
$\vert$ MMPG                    $\vert$          32.88     11.88      21.96          $\vert$        30.73    10.29    21.25        $\vert$         32.69    11.78    22.27         $\vert$
$\vert$ MMPG+Aspect             $\vert$          33.97     12.43      22.21          $\vert$        31.81    10.87    21.32        $\vert$         33.67    12.44    22.31         $\vert$
$\vert$ MMPG+Aspect+Consistency $\vert$          34.36     12.52      22.35          $\vert$        31.93    11.09    21.54        $\vert$         33.78    12.51    22.43         $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-FuX.1070.pdf}{Document Summarization with VHTM:Variational Hierarchical Topic-Aware Mechanism}}

\begin{quote}
Author: Xiyan Fu, Jun Wang, Jinghan Zhang, Jinmao Wei, Zhenglu Yang 
Dataset: CNN/DM
Score:
$\vert$ Model $\vert$  R-1  $\vert$  R-2  $\vert$  R-L  $\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ VHTM  $\vert$ 40.57 $\vert$ 18.05 $\vert$ 37.18 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-ChenY.1087.pdf}{TemPEST: Soft Template-based Personalized EDM Subject Generation Through Collaborative Summarization}}

\begin{quote}
Author: Yu-Hsiu Chen, Pin-Yu Chen, Hong-Han Shuai, Wen-Chih Peng
Dataset: KKday (A new dataset from KKday.com)
Score:

$\vert$ Models $\vert$  B-1  $\vert$  B-2  $\vert$  B-3  $\vert$ B-4  $\vert$  R-1  $\vert$  R-L  $\vert$
$\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$:---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ TemPEST  $\vert$ 24.36 $\vert$ 5.31 $\vert$ 3.15 $\vert$1.97 $\vert$ 65.47 $\vert$ 8.33 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-ZhuJ.1133.pdf}{Multimodal Summarization with Guidance of Multimodal Reference}}

\begin{quote}
Author: Junnan Zhu, Yu Zhou, Jiajun Zhang, Haoran Li, Chengqing Zong, hangliang Li 
Dataset: MSMO
Score:

$\vert$ Models      $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$ M\emph{sim $\vert$ IP    $\vert$  IPR  $\vert$ IPO   $\vert$ MR    $\vert$ MRR   $\vert$ MRO   $\vert$ AE    $\vert$ AE++  $\vert$
$\vert$ ----------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ :---: $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ MOF(PR}ENC) $\vert$ 41.05 $\vert$ 18.29 $\vert$ 37.74 $\vert$ 26.23 $\vert$ 62.63 $\vert$ 67.85 $\vert$ -     $\vert$ 57.13 $\vert$ 59.26 $\vert$ -     $\vert$ 66.52 $\vert$ 68.68 $\vert$
$\vert$ MOF(PR\emph{DEC) $\vert$ 41.20 $\vert$ 18.33 $\vert$ 37.80 $\vert$ 26.38 $\vert$ 65.45 $\vert$ 68.62 $\vert$ -     $\vert$ 58.38 $\vert$ 59.58 $\vert$ -     $\vert$ 67.02 $\vert$ 69.66 $\vert$
$\vert$ MOF(OR}ENC) $\vert$ 41.16 $\vert$ 18.35 $\vert$ 37.85 $\vert$ 26.15 $\vert$ 63.55 $\vert$   -   $\vert$ 68.76 $\vert$ 57.66 $\vert$ -     $\vert$ 59.55 $\vert$ 66.69 $\vert$ 69.04 $\vert$
$\vert$ MOF(OR\_DEC) $\vert$ 40.95 $\vert$ 18.12 $\vert$ 37.75 $\vert$ 26.30 $\vert$ 64.00 $\vert$   -   $\vert$ 71.78 $\vert$ 58.16 $\vert$ -     $\vert$ 60.58 $\vert$ 66.76 $\vert$ 69.24 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-LiH.1493.pdf}{Keywords-Guided Abstractive Sentence Summarization}}

\begin{quote}
Author: Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong, Xiaodong He 
Github: None
Dataset: English Gigaword dataset
Score:

$\vert$ Method                                                       $\vert$ R-1                         $\vert$ R-2                         $\vert$ R-L                         $\vert$
$\vert$ ------------------------------------------------------------ $\vert$ --------------------------- $\vert$ --------------------------- $\vert$ --------------------------- $\vert$
$\vert$ S2S-Sentence                                                 $\vert$ 45.09                       $\vert$ 23.58                       $\vert$ 42.37                       $\vert$
$\vert$ S2S-Keywords                                                 $\vert$ 44.85                       $\vert$ 20.54                       $\vert$ 41.61                       $\vert$
$\vert$ S2S-Sentence\&Keywords                                        $\vert$ 46.14                       $\vert$ 24.07                       $\vert$ 43.30                       $\vert$
$\vert$ Self-Selective Concat\texttt{$<$br /$>$}Self-Selective Gated\texttt{$<$br /$>$}Self-Selective Hier $\vert$ 46.23\texttt{$<$br /$>$}46.44\texttt{$<$br /$>$}46.51 $\vert$ 24.13\texttt{$<$br /$>$}24.31\texttt{$<$br /$>$}24.32 $\vert$ 43.26\texttt{$<$br /$>$}43.44\texttt{$<$br /$>$}43.45 $\vert$
$\vert$ Keywords-Selective Concat\texttt{$<$br /$>$}Keywords-Selective Gated\texttt{$<$br /$>$}Keywords-Selective Hier $\vert$ 46.32\texttt{$<$br /$>$}46.70\texttt{$<$br /$>$}46.72 $\vert$ 24.11\texttt{$<$br /$>$}24.48\texttt{$<$br /$>$}24.50 $\vert$ 43.38\texttt{$<$br /$>$}43.59\texttt{$<$br /$>$}43.81 $\vert$
$\vert$ Co-Selective Concat\texttt{$<$br /$>$}Co-Selective Gated\texttt{$<$br /$>$}Co-Selective Hier $\vert$ 46.53\texttt{$<$br /$>$}46.71\texttt{$<$br /$>$}46.80 $\vert$ 24.15\texttt{$<$br /$>$}24.53\texttt{$<$br /$>$}27.75 $\vert$ 43.24\texttt{$<$br /$>$}43.63\texttt{$<$br /$>$}43.83 $\vert$
$\vert$ Co-Selective Concat + PG\texttt{$<$br /$>$}Co-Selective Gated + PG\texttt{$<$br /$>$}Co-Selective Hier+ PG $\vert$ 46.68\texttt{$<$br /$>$}46.91\texttt{$<$br /$>$}46.93 $\vert$ 24.33\texttt{$<$br /$>$}24.61\texttt{$<$br /$>$}24.83 $\vert$ 43.31\texttt{$<$br /$>$}43.71\texttt{$<$br /$>$}43.92 $\vert$
$\vert$ Co-Selective Concat + DuakPG\texttt{$<$br /$>$}Co-Selective Gated+ DuakPG\texttt{$<$br /$>$}Co-Selective Hier+ DuakPG $\vert$ 47.05\texttt{$<$br /$>$}47.13\texttt{$<$br /$>$}47.14 $\vert$ 24.39\texttt{$<$br /$>$}24.87\texttt{$<$br /$>$}25.06 $\vert$ 43.77\texttt{$<$br /$>$}44.34\texttt{$<$br /$>$}44.39 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/1911.09844.pdf}{Weakly-Supervised Opinion Summarization by Leveraging External Information}}

\begin{quote}
Author: Chao Zhao Snigdha Chaturvedi 
Dataset: OPOSUM
Score:

$\vert$ Methods       $\vert$ R-1  $\vert$ R-2  $\vert$
$\vert$ ------------- $\vert$ :--: $\vert$ :--: $\vert$
$\vert$ ASPMENSUM     $\vert$ 46.6 $\vert$ 25.7 $\vert$
$\vert$ w/o filtering $\vert$ 48.0 $\vert$ 28.7 $\vert$
$\vert$ w/o Relevance $\vert$ 41.5 $\vert$ 20.5 $\vert$
$\vert$ w/o Sentiment $\vert$ 40.5 $\vert$ 18.2 $\vert$
$\vert$ w/o ILP       $\vert$ 46.2 $\vert$ 25.1 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-YangM.3953.pdf}{Be Relevant, Non-redundant, Timely: Deep Reinforcement Learning for Real-time Event Summarization}}

\begin{quote}
Author: Min Yang, Chengming Li, Fei Sun, Zhou Zhao, Ying Shen, Chenglin Wu 
Dataset: TREC-RTS-16, TREC-RTS-17
Scores: 
Event summarization results on TREC-RTS-16
$\vert$ Method    $\vert$ EG-0  $\vert$ nCG-0 $\vert$ EG-1  $\vert$ nCG-1 $\vert$ GMP    $\vert$ Latency $\vert$
$\vert$ --------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ------ $\vert$ ------- $\vert$
$\vert$ DRES      $\vert$ 0.087 $\vert$ 0.102 $\vert$ 0.308 $\vert$ 0.315 $\vert$ -0.080 $\vert$ 72264   $\vert$
$\vert$ DRES+Bert $\vert$ 0.089 $\vert$ 0.105 $\vert$ 0.306 $\vert$ 0.311 $\vert$ -0.078 $\vert$ 71983   $\vert$

Event summarization results on TREC-RTS-17
$\vert$ Method    $\vert$ EG-0  $\vert$ nCG-0 $\vert$ EG-1  $\vert$ nCG-1 $\vert$ GMP    $\vert$ Latency $\vert$
$\vert$ --------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ------ $\vert$ ------- $\vert$
$\vert$ DRES      $\vert$ 0.086 $\vert$ 0.075 $\vert$ 0.302 $\vert$ 0.278 $\vert$ -0.056 $\vert$ 35319   $\vert$
$\vert$ DRES+Bert $\vert$ 0.085 $\vert$ 0.077 $\vert$ 0.298 $\vert$ 0.283 $\vert$ -0.061 $\vert$ 35362   $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-CaoY.7050.pdf}{MultiSumm: Towards a Unified Model for Multi-Lingual Abstractive Summarization}}

\begin{quote}
Author: Yue Cao, Xiaojun Wan, Jin-ge Yao,Dian Yu
Dataset: A multi-lingual Dataset ( Monolingual Dataset, Machine Translation Dataset, Gigaword Dataset and LCSTS dataset, Dataset Construction for Bosnian and Croatian and contained)
Scores:

$\vert$ Method    $\vert$ Metric  $\vert$ Rich-Resource\texttt{$<$br /$>$}                         De              En              Es               Fr              Zh $\vert$ Low-Resource\texttt{$<$br /$>$}                   Bs           Hr $\vert$
$\vert$ --------- $\vert$ ------- $\vert$ ------------------------------------------------------------ $\vert$ ---------------------------------------------------- $\vert$
$\vert$ MultiSumm $\vert$ Rouge-1 $\vert$ 43.41        36.87         31.18        27.20         35.71  $\vert$ 22.47     23.04                                      $\vert$
$\vert$ MultiSumm $\vert$ Rouge-2 $\vert$ 21.86        17.96         12.24        11.78         21.86  $\vert$ 8.35       8.75                                      $\vert$
$\vert$ MultiSumm $\vert$ Rouge-L $\vert$ 39.77        33.07         26.22        23.57         33.61  $\vert$ 19.42     19.63                                      $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-JinH.7203.pdf}{SemSUM: Semantic Dependency Guided Neural Abstractive Summarization}}

\begin{quote}
Author: Hanqi Jin, Tianming Wang,Xiaojun Wan
Dataset: Gigaword, DUC2004, MSR-ATC
Score:

ROUGE F1, WMD unigram and BERTScore F1 evaluation on the Gigaword test set

$\vert$ Model                             $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ WMD   $\vert$ BERT  $\vert$
$\vert$ --------------------------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ MASS(Song et al. 2019)            $\vert$ 38.73 $\vert$ 19.71 $\vert$ 35.96 $\vert$ 34.28 $\vert$ 61.56 $\vert$
$\vert$ BiSET(Wang, Quan, and Wang 2019b) $\vert$ 39.11 $\vert$ 19.78 $\vert$ 16.87 $\vert$ 33.79 $\vert$ 61.24 $\vert$
$\vert$ \textbf{SemSUM}                        $\vert$ 38.89 $\vert$ 19.75 $\vert$ 36.09 $\vert$ 34.39 $\vert$ 61.56 $\vert$

ROUGE recall, WMD unigram and BERTScore F1 evaluation on the DUC2004 test set

$\vert$ Model      $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ WMD   $\vert$ BERT  $\vert$
$\vert$ ---------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ \textbf{SemSUM} $\vert$ 31.00 $\vert$ 11.11 $\vert$ 26.94 $\vert$ 26.71 $\vert$ 57.99 $\vert$

ROUGE F1, WMD unigram and BERTScore F1 evaluation on the MSR-ATC test set

$\vert$ Model      $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ WMD   $\vert$ BERT  $\vert$
$\vert$ ---------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ \textbf{SemSUM} $\vert$ 33.82 $\vert$ 17.08 $\vert$ 30.62 $\vert$ 17.14 $\vert$ 56.19 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/1911.10390.pdf}{Controlling the Amount of Verbatim Copying in Abstractive Summarization}}

\begin{quote}
Author: Kaiqiang Song, Bingqing Wang,Zhe Feng, Liu Ren, Fei Liu

Dataset: Gigaword dataset, Newsroom dataset
Score:
Summarization results on the Gigaword test set

$\vert$ Model                  $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ Bert-S $\vert$
$\vert$ ---------------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ------ $\vert$
$\vert$ Beam + BPNorm (c=0.55) $\vert$ 39.19 $\vert$ 20.38 $\vert$ 36.69 $\vert$ 61.46  $\vert$
$\vert$ Beam + SBWR(r=0.25)    $\vert$ 39.08 $\vert$ 20.47 $\vert$ 36.68 $\vert$ 61.51  $\vert$

ROUGE F1 and BERTScore F1 evaluation on the NewSroom test set

$\vert$ Model    $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ Bert-S $\vert$
$\vert$ -------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ------ $\vert$
$\vert$ pure-ext $\vert$ 43.21 $\vert$ 21.81 $\vert$ 40.05 $\vert$ 63.68 $\vert$
$\vert$ best-abs $\vert$ 45.93 $\vert$ 24.14 $\vert$ 42.51 $\vert$ 66.20 $\vert$
ROUGE F1, WMD unigram and BERTScore F1 evaluation on the NewSroom test set
$\vert$ Model    $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ Bert-S $\vert$
$\vert$ -------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ------ $\vert$
$\vert$ pure-ext $\vert$ 43.21 $\vert$ 21.81 $\vert$ 40.05 $\vert$ 63.68  $\vert$
$\vert$ best-abs $\vert$ 45.93 $\vert$ 24.14 $\vert$ 42.51 $\vert$ 66.20  $\vert$

ROUGE F1 and BERTScore F1 evaluation on the Gigaword test set

$\vert$ Model    $\vert$ RG-1  $\vert$ RG-2  $\vert$ RG-L  $\vert$ Bert-S $\vert$
$\vert$ -------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ------ $\vert$
$\vert$ pure-ext $\vert$ 39.44 $\vert$ 19.32 $\vert$ 36.10 $\vert$ 61.00  $\vert$
$\vert$ best-abs $\vert$ 40.89 $\vert$ 19.11 $\vert$ 37.60 $\vert$ 62.74  $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/1911.10389.pdf}{Joint Parsing and Generation for Abstractive Summarization}}

\begin{quote}
Author: Kaiqiang Song, Logan Lebanoff, Qipeng Guo,Xipeng Qiu, Xiangyang Xue, Chen Li, Dong Yu, Fei Liu
Dataset: Gigaword, NewSroom, CNN/DM, WEBMERGE
Scores:

Summarization on Gigaword dataset

$\vert$ Methods       $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ GenParse-Base $\vert$ 35.21 $\vert$ 17.10 $\vert$ 32.88 $\vert$
$\vert$ GenParse-Full $\vert$ 36.61 $\vert$ 18.85 $\vert$ 34.33 $\vert$

Summarization results on Newsroom, CNN/DM-R, and WebMerge datasets.

$\vert$ Dataset  $\vert$ System                           $\vert$ Rouge-1\texttt{$<$br /$>$}R            R              F           $\vert$ Rouge-2\texttt{$<$br /$>$}P                R           F          $\vert$ Rouge-L\texttt{$<$br /$>$}R              R            F           $\vert$
$\vert$ -------- $\vert$ -------------------------------- $\vert$ ---------------------------------------------------- $\vert$ ---------------------------------------------------- $\vert$ ---------------------------------------------------- $\vert$
$\vert$ NEWSROOM $\vert$ GenParse-Base\texttt{$<$br /$>$}GenParse-Full $\vert$ 41.88    36.00    37.65\texttt{$<$br /$>$}45.17    39.77    41.06 $\vert$ 20.04    16.90    17.70\texttt{$<$br /$>$}23.48    20.17    20.89 $\vert$ 38.73    33.33    34.84\texttt{$<$br /$>$}41.82    36.81    38.01 $\vert$
$\vert$ CNN/DM   $\vert$ GenParse-Base\texttt{$<$br /$>$}GenParse-Full $\vert$ 48.24    46.52    46.46\texttt{$<$br /$>$}50.15    53.11    50.49 $\vert$ 31.44    29.62    29.82\texttt{$<$br /$>$}34.51    35.99    34.38 $\vert$ 45.43    43.72    43.71\texttt{$<$br /$>$}47.33    50.00    47.60 $\vert$
$\vert$ WEBMERGE $\vert$ GenParse-Base\texttt{$<$br /$>$}GenParse-Full $\vert$ 37.79    35.86    36.23\texttt{$<$br /$>$}62.26    54.69    57.24 $\vert$ 12.63    11.99    12.09\texttt{$<$br /$>$}32.10    28.41    29.58 $\vert$ 28.87    27.59    27.77\texttt{$<$br /$>$}48.13    42.54    44.41 $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/AAAI-XiaoL.9028.pdf}{Copy or Rewrite: Hybrid Summarization withHierarchical Reinforcement Learning}}

\begin{quote}
Author: Liqiang Xiao, Lu Wang, Hao He, Yaohui Jin
Datasets: CNN/DM
Scores:

$\vert$ Models                                    $\vert$  R-1  $\vert$  R-2  $\vert$  R-L  $\vert$ METEOR $\vert$
$\vert$ ----------------------------------------- $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :----: $\vert$
$\vert$ Rewrite + EXT-RL                          $\vert$ 40.73 $\vert$ 17.85 $\vert$ 38.26 $\vert$ 19.05  $\vert$
$\vert$ Copy + EXT-RL                             $\vert$ 41.48 $\vert$ 18.75 $\vert$ 37.79 $\vert$ 21.71  $\vert$
$\vert$ Copy/Rewrite + Unbalanced Marker + EXT-RL $\vert$ 42.10 $\vert$ 18.91 $\vert$ 38.87 $\vert$ 20.69  $\vert$
$\vert$ Copy/Rewrite + History + EXT-RL           $\vert$ 41.98 $\vert$ 18.96 $\vert$ 38.83 $\vert$ 21.91  $\vert$
$\vert$ Copy/Rewrite + History + INDEPENDENT-RL   $\vert$ 42.21 $\vert$ 18.91 $\vert$ 38.94 $\vert$ 21.16  $\vert$
$\vert$ Copy/Rewrite + History + HRL              $\vert$ 42.46 $\vert$ 19.10 $\vert$ 39.19 $\vert$ 21.88  $\vert$
$\vert$ Copy/Rewrite + History + HRL + BERT       $\vert$ 42.92 $\vert$ 19.43 $\vert$ 39.35 $\vert$ 22.12  $\vert$
\end{quote}

\subsubsection*{AAAI 2020: \href{./paper/2002.03740.pdf}{Convolutional Hierarchical Attention Network for Query-Focused Video Summarization}}

\begin{quote}
Author: Shuwen Xiao, Zhou Zhao, Zijian Zhang, Xiaohui Yan, Min Yang
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2004.04228.pdf}{Asking and Answering Questions to Evaluate the Factual Consistency of Summaries}}

\begin{quote}
Author: Paulus, Romain, Caiming Xiong, and Richard Socher 
Datasets: CNN/DM, XSUM
Scores: 

$\vert$ Metric    $\vert$ CNN/DM $\vert$ XSUM  $\vert$
$\vert$ --------- $\vert$ ------ $\vert$ ----- $\vert$
$\vert$ ROUGE-1   $\vert$ 28.74  $\vert$ 13.22 $\vert$
$\vert$ ROUGE-2   $\vert$ 17.72  $\vert$ 8.95  $\vert$
$\vert$ ROUGE-L   $\vert$ 24.09  $\vert$ 8.86  $\vert$
$\vert$ METEOR    $\vert$ 26.65  $\vert$ 10.03 $\vert$
$\vert$ BLEU-1    $\vert$ 29.68  $\vert$ 11.76 $\vert$
$\vert$ BLEU-2    $\vert$ 26.65  $\vert$ 11.68 $\vert$
$\vert$ BLEU-3    $\vert$ 23.96  $\vert$ 8.41  $\vert$
$\vert$ BLEU-4    $\vert$ 21.45  $\vert$ 5.64  $\vert$
$\vert$ BERTScore $\vert$ 27.63  $\vert$ 2.51  $\vert$
$\vert$ QAGS      $\vert$ 54.53  $\vert$ 17.49 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.121.pdf}{Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization}}

\begin{quote}
Author: Junnan Zhu, Yu Zhou, Jiajun Zhang and Chengqing Zong
Datasets: En2ZhSum, Zh2EnSum
Score: 
Rouge F1 Scores and MoverScore scores on Zh2EnSum test set
$\vert$ Methods $\vert$ Model $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$ MVS   $\vert$
$\vert$ ------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ ATS     $\vert$ Naive $\vert$ 40.40 $\vert$ 23.82 $\vert$ 36.63 $\vert$ 21.86 $\vert$
$\vert$ ATS     $\vert$ Equal $\vert$ 40.10 $\vert$ 23.36 $\vert$ 36.22 $\vert$ 21.41 $\vert$
$\vert$ ATS     $\vert$ Adapt $\vert$ 40.86 $\vert$ 24.12 $\vert$ 36.97 $\vert$ 22.15 $\vert$

Rouge F1 Scores and MoverScore scores on En2ZhSum test set
$\vert$ Methods $\vert$ Model $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ ATS     $\vert$ Naive $\vert$ 40.19 $\vert$ 21.84 $\vert$ 36.46 $\vert$
$\vert$ ATS     $\vert$ Equal $\vert$ 39.98 $\vert$ 21.63 $\vert$ 36.29 $\vert$
$\vert$ ATS     $\vert$ Adapt $\vert$ 40.47 $\vert$ 22.21 $\vert$ 36.89 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.451.pdf}{Discourse-Aware Neural Extractive Text Summarization}}

\begin{quote}
Author: Jiacheng Xu, Zhe Gan, Yu Cheng and Jingjing Liu
Datasets: CNN/DM, New York Times
Scores:

Results on the test set of the CNNDM dataset.
$\vert$ Methods           $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ----------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ DISCOBERT W.GC    $\vert$ 43.58 $\vert$ 20.64 $\vert$ 40.42 $\vert$
$\vert$ DISCOBERT W.GR    $\vert$ 43.68 $\vert$ 20.71 $\vert$ 40.54 $\vert$
$\vert$ DISCOBERT W.GR\&GC $\vert$ 43.77 $\vert$ 20.85 $\vert$ 40.67 $\vert$

Results on the test set of the NYT dataset

$\vert$ Methods           $\vert$  R-1  $\vert$  R-2  $\vert$  R-L  $\vert$
$\vert$ ----------------- $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ DISCOBERT W.GC    $\vert$ 49.79 $\vert$ 30.18 $\vert$ 42.48 $\vert$
$\vert$ DISCOBERT W.GR    $\vert$ 49.86 $\vert$ 30.25 $\vert$ 42.55 $\vert$
$\vert$ DISCOBERT W.GR\&GC $\vert$ 50.00 $\vert$ 30.38 $\vert$ 42.70 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.01791.pdf}{Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction}}

\begin{quote}
Author: Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova and Katja Markert
Datasets: DUC2004, Gigaword
Scores:

Results for headline generation on the Gigaword test set.

$\vert$           Model            $\vert$      Data       $\vert$ Len D $\vert$  R-1  $\vert$  R-2  $\vert$  R-L  $\vert$
$\vert$ :------------------------: $\vert$ :-------------: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$        hc\emph{article}8        $\vert$     article     $\vert$   8   $\vert$ 23.09 $\vert$ 7.50  $\vert$ 21.29 $\vert$
$\vert$         hc\emph{title}8         $\vert$      title      $\vert$   8   $\vert$ 26.32 $\vert$ 9.63  $\vert$ 24.19 $\vert$
$\vert$       hc\emph{article}10        $\vert$     article     $\vert$  10   $\vert$ 24.44 $\vert$ 8.01  $\vert$ 22.21 $\vert$
$\vert$ hc\emph{article}10 + twitter\emph{10 $\vert$ article+twitter $\vert$  10   $\vert$ 28.26 $\vert$ 10.42 $\vert$ 25.43 $\vert$
$\vert$        hc}title\emph{10         $\vert$      title      $\vert$  10   $\vert$ 27.52 $\vert$ 10.27 $\vert$ 24.91 $\vert$
$\vert$   hc}title\emph{10+billon}10    $\vert$  title+billon   $\vert$  10   $\vert$ 28.80 $\vert$ 10.66 $\vert$ 25.82 $\vert$
$\vert$       hc\emph{article}50p       $\vert$     article     $\vert$  50\%  $\vert$ 25.58 $\vert$ 8.44  $\vert$ 22.66 $\vert$
$\vert$        hc\emph{title}50p        $\vert$      title      $\vert$  50\%  $\vert$ 27.05 $\vert$ 9.75  $\vert$ 23.89 $\vert$

Results for headline generation on the DUC2004 test set.

$\vert$         Model         $\vert$  R-1  $\vert$  R-2  $\vert$  R-L  $\vert$
$\vert$ :-------------------: $\vert$ :---: $\vert$ :---: $\vert$ :---: $\vert$
$\vert$ HC\emph{article}13 $\vert$24.21 $\vert$ 6.63 $\vert$ 21.24 $\vert$
$\vert$ HC\emph{title}13 $\vert$ 26.04 $\vert$ 8.06 $\vert$ 22.90 $\vert$
$\vert$ HC\emph{title + twitter}13 $\vert$ 27.41 $\vert$ 8.76 $\vert$ 23.89 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.10107.pdf}{Examining the State-of-the-Art in News Timeline Summarization}}

\begin{quote}
Author: Demian Gholipour Ghalandari and Georgiana Ifrim
Datasets: T17 Dataset, CRISIS Dataset, ENTITIES Dataset
Scores:

$\vert$                  $\vert$ T-17      $\vert$ T-17 $\vert$ T-17 $\vert$ CRISIS $\vert$ CRISIS $\vert$ CRISIS $\vert$ ENTITIES $\vert$ ENTITIES $\vert$ ENTITIES $\vert$
$\vert$ :--------------: $\vert$ :----------: $\vert$ :--: $\vert$ :--: $\vert$ :--: $\vert$ :--: $\vert$ :--: $\vert$ :--: $\vert$ :--: $\vert$ :--: $\vert$
$\vert$ Method           $\vert$ AR1-F $\vert$ AR2-F $\vert$ Date-F1 $\vert$ AR1-F $\vert$ AR2-F $\vert$ Date-F1 $\vert$ AR1-F $\vert$ AR2-F $\vert$ Date-F1 $\vert$
$\vert$ DATEWISE         $\vert$ 0.12 $\vert$ 0.035 $\vert$ 0.544 $\vert$ 0.089 $\vert$ 0.026 $\vert$ 0.295 $\vert$ 0.057 $\vert$ 0.017 $\vert$ 0.205 $\vert$
$\vert$ DATEWISE(titles) $\vert$ - $\vert$ - $\vert$ - $\vert$ 0.072 $\vert$ 0.016 $\vert$ 0.287 $\vert$ 0.057 $\vert$ 0.017 $\vert$ 0.194 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2004.08795.pdf}{Extractive Summarization as Text Matching}}

\begin{quote}
Author: Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu and Xuanjing Huang
Datasets: CNN/DM, Reddit, XSUM, WikiHow, PubMed and Multi-News
Scores: 

Results on CNN/DM test set

$\vert$ Model                   $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ----------------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ BERT EXT                $\vert$ 42.73 $\vert$ 20.13 $\vert$ 39.20 $\vert$
$\vert$ BERT EXT + Tri-Blocking $\vert$ 43.18 $\vert$ 20.16 $\vert$ 39.56 $\vert$
$\vert$ MATCHSUM (BERT-base)    $\vert$ 44.22 $\vert$ 20.62 $\vert$ 40.38 $\vert$
$\vert$ MATCHSUM (ROBERTa-base) $\vert$ 44.41 $\vert$ 20.86 $\vert$ 40.55 $\vert$

Results on test sets of Reddit 

$\vert$ Model                $\vert$ R-1   $\vert$ R-2  $\vert$ R-L   $\vert$
$\vert$ -------------------- $\vert$ ----- $\vert$ ---- $\vert$ ----- $\vert$
$\vert$ BERT EXT (num = 1)   $\vert$ 21.99 $\vert$ 5.21 $\vert$ 16.99 $\vert$
$\vert$ BERT EXT (num = 2)   $\vert$ 23.86 $\vert$ 5.85 $\vert$ 19.11 $\vert$
$\vert$ MATCHSUM (Sel = 1)   $\vert$ 22.87 $\vert$ 5.15 $\vert$ 17.40 $\vert$
$\vert$ MATCHSUM (Sel = 2)   $\vert$ 24.90 $\vert$ 5.91 $\vert$ 20.03 $\vert$
$\vert$ MATCHSUM (Sel = 1,2) $\vert$ 25.09 $\vert$ 6.17 $\vert$ 20.13 $\vert$

Results on test sets of XSUM

$\vert$ Model                $\vert$ R-1   $\vert$ R-2  $\vert$ R-L   $\vert$
$\vert$ -------------------- $\vert$ ----- $\vert$ ---- $\vert$ ----- $\vert$
$\vert$ BERT EXT (num = 1)   $\vert$ 22.53 $\vert$ 4.36 $\vert$ 16.23 $\vert$
$\vert$ BERT EXT (num = 2)   $\vert$ 22.86 $\vert$ 4.48 $\vert$ 17.16 $\vert$
$\vert$ MATCHSUM (Sel = 1)   $\vert$ 23.35 $\vert$ 4.46 $\vert$ 16.71 $\vert$
$\vert$ MATCHSUM (Sel = 2)   $\vert$ 24.48 $\vert$ 4.58 $\vert$ 18.31 $\vert$
$\vert$ MATCHSUM (Sel = 1,2) $\vert$ 24.86 $\vert$ 4.66 $\vert$ 18.41 $\vert$

Results on test sets of WikiHow, PubMed and Multi-News

​																						 			WikiHow	                                          PubMed                                         Multi- News

$\vert$ Model                $\vert$ R-1   $\vert$ R-2  $\vert$ R-L   $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$ R-1   $\vert$ R-2   $\vert$ R-1   $\vert$
$\vert$ -------------------- $\vert$ ----- $\vert$ ---- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ BERTEXT              $\vert$ 30.31 $\vert$ 8.71 $\vert$ 28.24 $\vert$ 41.05 $\vert$ 14.88 $\vert$ 36.57 $\vert$ 45.80 $\vert$ 16.42 $\vert$ 41.53 $\vert$
$\vert$ + 3gram-Blocking     $\vert$ 30.37 $\vert$ 8.45 $\vert$ 28.28 $\vert$ 38.81 $\vert$ 13.62 $\vert$ 34.52 $\vert$ 44.94 $\vert$ 15.47 $\vert$ 40.63 $\vert$
$\vert$ + 4gram-Blocking     $\vert$ 30.40 $\vert$ 8.67 $\vert$ 28.32 $\vert$ 40.29 $\vert$ 14.37 $\vert$ 35.88 $\vert$ 45.86 $\vert$ 16.23 $\vert$ 41.57 $\vert$
$\vert$ MATCHSUM (BERT-base) $\vert$ 31.85 $\vert$ 8.98 $\vert$ 29.58 $\vert$ 41.21 $\vert$ 14.91 $\vert$ 36.75 $\vert$ 46.20 $\vert$ 16.51 $\vert$ 41.89 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/1908.10383.pdf}{Facet-Aware Evaluation for Extractive Summarization}}

\begin{quote}
Author: Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren and Jiawei Han
Datasets: CNN/DM
Scores:

Performance comparison of extractive methods under ROUGE F1 and Facet-Aware Recall (FAR).

$\vert$ Method        $\vert$ R-1  $\vert$ R-2  $\vert$ R-L  $\vert$ FAR  $\vert$
$\vert$ ------------- $\vert$ ---- $\vert$ ---- $\vert$ ---- $\vert$ ---- $\vert$
$\vert$ Lead-3        $\vert$ 41.9 $\vert$ 19.6 $\vert$ 34.8 $\vert$ 50.6 $\vert$
$\vert$ FastRL(E)     $\vert$ 41.6 $\vert$ 20.3 $\vert$ 35.5 $\vert$ 50.8 $\vert$
$\vert$ BanditSum     $\vert$ 42.7 $\vert$ 20.2 $\vert$ 35.8 $\vert$ 44.7 $\vert$
$\vert$ NeuSum        $\vert$ 42.7 $\vert$ 22.1 $\vert$ 36.4 $\vert$ 51.2 $\vert$
$\vert$ Refresh       $\vert$ 42.8 $\vert$ 20.3 $\vert$ 39.3 $\vert$ 51.3 $\vert$
$\vert$ UnifiedSum(E) $\vert$ 42.6 $\vert$ 20.1 $\vert$ 35.5 $\vert$ 54.8 $\vert$
$\vert$ Oracle        $\vert$ 53.8 $\vert$ 32.1 $\vert$ 48.1 $\vert$ 84.8 $\vert$

ROUGE-1 F1 of extractive and abstractive methods on noisy (N), low abstraction (L), high abstraction (H), and high quality (L + H) samples

The first 6 methods belong to Extractive Summarization, and the rest are Abstractive.

$\vert$ Method          $\vert$ N    $\vert$ L    $\vert$ H    $\vert$ L+H  $\vert$
$\vert$ --------------- $\vert$ ---- $\vert$ ---- $\vert$ ---- $\vert$ ---- $\vert$
$\vert$ Lead-3          $\vert$ 34.1 $\vert$ 41.9 $\vert$ 24.9 $\vert$ 38.9 $\vert$
$\vert$ FastRL(E)       $\vert$ 33.5 $\vert$ 41.6 $\vert$ 31.2 $\vert$ 39.8 $\vert$
$\vert$ BanditSum       $\vert$ 35.3 $\vert$ 42.7 $\vert$ 34.1 $\vert$ 41.2 $\vert$
$\vert$ NeuSum          $\vert$ 34.9 $\vert$ 42.7 $\vert$ 30.7 $\vert$ 40.6 $\vert$
$\vert$ Refresh         $\vert$ 35.7 $\vert$ 42.8 $\vert$ 32.2 $\vert$ 40.9 $\vert$
$\vert$ UnifiedSum(E)   $\vert$ 34.2 $\vert$ 42.6 $\vert$ 31.3 $\vert$ 40.6 $\vert$
$\vert$ PG              $\vert$ 32.6 $\vert$ 40.6 $\vert$ 27.5 $\vert$ 38.2 $\vert$
$\vert$ FastRL(E+A)     $\vert$ 35.1 $\vert$ 40.8 $\vert$ 29.9 $\vert$ 38.8 $\vert$
$\vert$ UnifiedSum(E+A) $\vert$ 34.2 $\vert$ 42.4 $\vert$ 29.2 $\vert$ 40.1 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2004.12393.pdf}{Heterogeneous Graph Neural Networks for Extractive Document Summarization}}

\begin{quote}
Author: Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu and Xuanjing Huang
Datasets: CNNDM NYT50 Milti-News
Scores: 

Performance comparison of the models on CNNDM dataset.

$\vert$ Model            $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ---------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ Ext-BiLSTM       $\vert$ 41.59 $\vert$ 19.03 $\vert$ 38.04 $\vert$
$\vert$ Ext-Transformer  $\vert$ 41.33 $\vert$ 18.83 $\vert$ 37.65 $\vert$
$\vert$ HSG              $\vert$ 42.31 $\vert$ 19.51 $\vert$ 38.74 $\vert$
$\vert$ HSG+Tri-Blocking $\vert$ 42.95 $\vert$ 19.76 $\vert$ 39.23 $\vert$

Performance comparison of the models on NYT50 dataset.

$\vert$ Model            $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ---------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ Ext-BiLSTM       $\vert$ 46.32 $\vert$ 25.84 $\vert$ 42.16 $\vert$
$\vert$ Ext-Transformer  $\vert$ 45.07 $\vert$ 24.72 $\vert$ 40.85 $\vert$
$\vert$ HSG              $\vert$ 46.89 $\vert$ 26.26 $\vert$ 42.58 $\vert$
$\vert$ HSG+Tri-Blocking $\vert$ 46.57 $\vert$ 25.94 $\vert$ 42.25 $\vert$

Performance comparison of the models on Multi-News dataset.

$\vert$ Model             $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ----------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ HSG               $\vert$ 45.66 $\vert$ 16.22 $\vert$ 41.80 $\vert$
$\vert$ HSG+Tri-Blocking  $\vert$ 44.92 $\vert$ 15.59 $\vert$ 40.89 $\vert$
$\vert$ HDSG              $\vert$ 46.05 $\vert$ 16.35 $\vert$ 42.08 $\vert$
$\vert$ HDSG+Tri-Blocking $\vert$ 45.55 $\vert$ 15.78 $\vert$ 41.29 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.554.pdf}{Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization}}

\begin{quote}
Author: Yue Cao, Hui Liu and Xiaojun Wan
Datasets: Gigaword DUC2004 LCSTS CNNDM
Scores:
Notice: \emph{g} means gigaword dataset, \emph{d} means DUC2004 and so on.

$\vert$ model $\vert$ g\emph{R1  $\vert$ g}R2  $\vert$ g\emph{RL  $\vert$ d}R1  $\vert$ d\emph{R2 $\vert$ d}RL  $\vert$ l\emph{R1  $\vert$ l}R2  $\vert$ l\emph{RL  $\vert$ c}R1  $\vert$ c\emph{R2  $\vert$ c}Rl  $\vert$
$\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ---- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ ours  $\vert$ 32.04 $\vert$ 13.60 $\vert$ 27.91 $\vert$ 27.25 $\vert$ 8.71 $\vert$ 23.36 $\vert$ 40.97 $\vert$ 23.20 $\vert$ 36.96 $\vert$ 39.12 $\vert$ 16.76 $\vert$ 33.86 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.01159.pdf}{Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward}}

\begin{quote}
Author: Luyang Huang, Lingfei Wu and Lu Wang
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.10043.pdf}{Leveraging Graph to Improve Abstractive Multi-Document Summarization}}

\begin{quote}
Author: Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang and Junping Du
Datasets: WikiSum Multi-News
Scores:
Performance of models on WikiSum dataset

$\vert$ Model            $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ---------------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ GraphSum         $\vert$ 42.63 $\vert$ 27.70 $\vert$ 36.97 $\vert$
$\vert$ GraphSum+RoBERTa $\vert$ 42.99 $\vert$ 27.83 $\vert$ 37.36 $\vert$

Performance of models on Multi-News datasets

$\vert$ Models                   $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ------------------------ $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ GraphSum                 $\vert$ 45.02 $\vert$ 16.69 $\vert$ 22.50 $\vert$
$\vert$ G.S.(Similarity)+RoBERTa $\vert$ 45.93 $\vert$ 17.33 $\vert$ 23.33 $\vert$
$\vert$ G.S.(Topic)+RoBERTa      $\vert$ 46.07 $\vert$ 17.42 $\vert$ 23.21 $\vert$
$\vert$ G.S.(Discourse)+RoBERTa  $\vert$ 45.87 $\vert$ 17.56 $\vert$ 23.39 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2004.12302.pdf}{MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization}}

\begin{quote}
Author: Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu and Chenliang Li
Contribution: released a new dataset
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.556.pdf}{Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization}}

\begin{quote}
Author: Hanqi Jin, Tianming Wang and Xiaojun Wan
Dataset: Multi-News dataset
Scores:

$\vert$ Models     $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ---------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ MGSum-ext  $\vert$ 44.75 $\vert$ 15.75 $\vert$ 19.30 $\vert$
$\vert$ MGSum-abs  $\vert$ 46.00 $\vert$ 16.81 $\vert$ 20.09 $\vert$
$\vert$ oracle ext $\vert$ 49.02 $\vert$ 29.78 $\vert$ 29.19 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.01619.pdf}{On Faithfulness and Factuality in Abstractive Summarization}}

\begin{quote}
Author: Joshua Maynez, Shashi Narayan, Bernd Bohnet and Ryan McDonald
Dataset: XSum
Scores:

$\vert$ Models  $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$ BERTScoreBERTS2S $\vert$
$\vert$ ------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ---------------- $\vert$
$\vert$ BERTS2S $\vert$ 38.42 $\vert$ 16.96 $\vert$ 31.27 $\vert$ 78.85            $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2004.12727.pdf}{Screenplay Summarization Using Latent Narrative Structure}}

\begin{quote}
Pinelopi Papalampidi, Frank Keller, Lea Frermann and Mirella Lapata
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/1911.02247.pdf}{Unsupervised Opinion Summarization as Copycat-Review Generation}}

\begin{quote}
Author: Arthur Bražinskas, Mirella Lapata and Ivan Titov
Dataset: Yelp Amazon
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2004.10150.pdf}{Unsupervised Opinion Summarization with Noising and Denoising}}

\begin{quote}
Author: Reinald Kim Amplayo and Mirella Lapata
Dataset: CNNDM 
Scores: 

$\vert$ Model     $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ --------- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ EDUSum+RL $\vert$ 40.89 $\vert$ 18.30 $\vert$ 37.79 $\vert$
$\vert$ EDUSum    $\vert$ 41.40 $\vert$ 18.03 $\vert$ 38.79 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.120.pdf}{A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal}}

\begin{quote}
Author: Demian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover and Georgiana Ifrim
Contribution: released a new dataset
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.00163.pdf}{Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization}}

\begin{quote}
Author: Sajad Sotudeh Gharebagh, Nazli Goharian and Ross Filice
Dataset: MIMIC-CXR Open-I
Scores:
performance of the model on MIMIC-CXR dataset

$\vert$ Model $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ ours  $\vert$ 53.57 $\vert$ 40.78 $\vert$ 51.81 $\vert$

performance of the model on Open-I dataset

$\vert$ Model $\vert$ R-1   $\vert$ R-2   $\vert$ R-L   $\vert$
$\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ ours  $\vert$ 40.88 $\vert$ 24.44 $\vert$ 40.37 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.551.pdf}{Composing Elementary Discourse Units in Abstractive Summarization}}

\begin{quote}
Author: Zhenwen Li, Wenhao Wu and Sujian Li
Dataset: Yelp Amazon
Scores:

$\vert$ Models                      $\vert$ R-1   $\vert$ R-2  $\vert$ R-L   $\vert$
$\vert$ --------------------------- $\vert$ ----- $\vert$ ---- $\vert$ ----- $\vert$
$\vert$ Copycat (on Yelp dataset)   $\vert$ 29.47 $\vert$ 5.26 $\vert$ 18.09 $\vert$
$\vert$ Copycat (on Amazon dataset) $\vert$ 31.97 $\vert$ 5.81 $\vert$ 20.16 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.01840.pdf}{Exploring Content Selection in Summarization of Novel Chapters}}

\begin{quote}
Faisal Ladhak, Bryan Li, Yaser Al-Onaizan and Kathy McKeown
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.455.pdf}{Fact-based Content Weighting for Evaluating Abstractive Summarisation}}

\begin{quote}
Xinnuo Xu, Ondřej Dušek, Jingyi Li, Verena Rieser and Ioannis Konstas
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.01901.pdf}{OpinionDigest: A Simple Framework for Opinion Summarization}}

\begin{quote}
Author: Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis and Wang-Chiew Tan
Dataset: YELP
Scores:

$\vert$ Model         $\vert$ R1    $\vert$ R2   $\vert$ RL    $\vert$
$\vert$ ------------- $\vert$ ----- $\vert$ ---- $\vert$ ----- $\vert$
$\vert$ OPINIONDIGEST $\vert$ 29.30 $\vert$ 5.77 $\vert$ 18.56 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2020.acl-main.125.pdf}{Self-Attention Guided Copy Mechanism for Abstractive Summarization}}

\begin{quote}
Author: Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He and Bowen Zhou
Dataset: CNNDM Gigaword
Scores:
Performance tested on the CNNDM

$\vert$ Model              $\vert$ R1    $\vert$ R2    $\vert$ RL    $\vert$
$\vert$ ------------------ $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ SAGCopy Outdegree  $\vert$ 42.53 $\vert$ 19.92 $\vert$ 39.44 $\vert$
$\vert$ SAGCopy Indegree-1 $\vert$ 42.30 $\vert$ 19.75 $\vert$ 39.23 $\vert$
$\vert$ SAGCopy Indegree-2 $\vert$ 42.56 $\vert$ 19.89 $\vert$ 39.40 $\vert$
$\vert$ SAGCopy Indegree-3 $\vert$ 42.34 $\vert$ 19.72 $\vert$ 39.29 $\vert$

Performance tested on the Gigaword

$\vert$ Model              $\vert$ R1    $\vert$ R2    $\vert$ RL    $\vert$
$\vert$ ------------------ $\vert$ ----- $\vert$ ----- $\vert$ ----- $\vert$
$\vert$ SAGCopy Outdegree  $\vert$ 38.86 $\vert$ 19.91 $\vert$ 36.06 $\vert$
$\vert$ SAGCopy Indegree-1 $\vert$ 38.84 $\vert$ 20.39 $\vert$ 36.27 $\vert$
$\vert$ SAGCopy Indegree-2 $\vert$ 38.70 $\vert$ 20.16 $\vert$ 36.09 $\vert$
$\vert$ SAGCopy Indegree-3 $\vert$ 38.69 $\vert$ 19.83 $\vert$ 35.98 $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2005.03724.pdf}{SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization}}

\begin{quote}
Author: Yang Gao, Wei Zhao and Steffen Eger
Dataset: TAC08 TAC09
Scores:

$\vert$ Model  $\vert$ TAC08\emph{R1 $\vert$ TAC08}R2 $\vert$ TAC08\emph{RL $\vert$ TAC09}R1 $\vert$ TAC09\emph{R2 $\vert$ TAC09}RL $\vert$
$\vert$ ------ $\vert$ -------- $\vert$ -------- $\vert$ -------- $\vert$ -------- $\vert$ -------- $\vert$ -------- $\vert$
$\vert$ NTD\_SP $\vert$ 37.6     $\vert$ 10.2     $\vert$ 29.6     $\vert$ 38.0     $\vert$ 10.3     $\vert$ 19.4     $\vert$
\end{quote}

\subsubsection*{ACL 2020: \href{./paper/2006.05621.pdf}{Understanding Points of Correspondence between Sentences for Abstractive Summarization}}

\begin{quote}
Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang and Fei Liu
\end{quote}
\end{quote}

\section*{2019}

\begin{quote}
\subsubsection*{ACL 2019: \href{./paper/P19-1098.pdf}{Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization}}

\begin{quote}
Author: Sangwoo Cho,  Logan Lebanoff ,  Hassan Foroosh,  Fei Liu
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1099.pdf}{Global Optimization under Length Constraint for Neural Text Summarization}}

\begin{quote}
Takuya Makino $\vert$ Tomoya Iwakura $\vert$ Hiroya Takamura $\vert$ Manabu Okumura
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1100.pdf}{Searching for Effective Neural Extractive Summarization: What Works and What’s Next}}

\begin{quote}
Author: Ming Zhong $\vert$ Pengfei Liu $\vert$ Danqing Wang $\vert$ Xipeng Qiu $\vert$ Xuanjing Huang
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1102.pdf}{Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model}}

\begin{quote}
Alexander Fabbri $\vert$ Irene Li $\vert$ Tianwei She $\vert$ Suyi Li $\vert$ Dragomir Radev
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1204.pdf}{TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks}}

\begin{quote}
Guy Lev $\vert$ Michal Shmueli-Scheuer $\vert$ Jonathan Herzig $\vert$ Achiya Jerbi $\vert$ David Konopnicki
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1205.pdf}{Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization}}

\begin{quote}
Yongjian You $\vert$ Weijia Jia $\vert$ Tianyi Liu $\vert$ Wenmian Yang
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1206.pdf}{Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking}}

\begin{quote}
Author: Masaru Isonuma $\vert$ Junichiro Mori $\vert$ Ichiro Sakata
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1207.pdf.pdf}{BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization}}

\begin{quote}
Author: Kai Wang $\vert$ Xiaojun Quan $\vert$ Rui Wang
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1209.pdf}{Scoring Sentence Singletons and Pairs for Abstractive Summarization}}

\begin{quote}
Author: Logan Lebanoff $\vert$ Kaiqiang Song $\vert$ Franck Dernoncourt $\vert$ Doo Soon Kim $\vert$ Seokhwan Kim $\vert$ Walter Chang $\vert$ Fei Liu
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1210.pdf}{Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization}}

\begin{quote}
Author: Manling Li $\vert$ Lingyu Zhang $\vert$ Heng Ji $\vert$ Richard J. Radke
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1212.pdf}{BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization}}

\begin{quote}
Author: Eva Sharma $\vert$ Chen Li $\vert$ Lu Wang
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1214.pdf}{Self-Supervised Learning for Contextualized Extractive Summarization}}

\begin{quote}
Author: Hong Wang $\vert$ Xin Wang $\vert$ Wenhan Xiong $\vert$ Mo Yu $\vert$ Xiaoxiao Guo $\vert$ Shiyu Chang $\vert$ William Yang Wang
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1215.pdf}{On the Summarization of Consumer Health Questions}}

\begin{quote}
Author: Asma Ben Abacha $\vert$ Dina Demner-Fushman
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1305.pdf}{Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention}}

\begin{quote}
Author: Xiangyu Duan $\vert$ Mingming Yin $\vert$ Min Zhang $\vert$ Boxing Chen $\vert$ Weihua Luo
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1330.pdf}{HighRES: Highlight-based Reference-less Evaluation of Summarization}}

\begin{quote}
Author: Hardy Hardy $\vert$ Shashi Narayan $\vert$ Andreas Vlachos
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1499.pdf}{HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization}}

\begin{quote}
Author: Xingxing Zhang $\vert$ Furu Wei $\vert$ Ming Zhou
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1500.pdf}{Hierarchical Transformers for Multi-Document Summarization}}

\begin{quote}
Author: Yang Liu $\vert$ Mirella Lapata
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1501.pdf}{Abstractive Text Summarization Based on Deep Learning and Semantic Content Generalization}}

\begin{quote}
Author: Panagiotis Kouris $\vert$ Georgios Alexandridis $\vert$ Andreas Stafylopatis
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1502.pdf}{Studying Summarization Evaluation Metrics in the Appropriate Scoring Range}}

\begin{quote}
Author: Maxime Peyrard
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1503.pdf}{Simple Unsupervised Summarization by Contextual Matching}}

\begin{quote}
Author: Jiawei Zhou $\vert$ Alexander Rush
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1628.pdf}{Sentence Centrality Revisited for Unsupervised Summarization}}

\begin{quote}
Author: Hao Zheng $\vert$ Mirella Lapata
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1630.pdf}{Inducing Document Structure for Aspect-based Summarization}}

\begin{quote}
Author: Lea Frermann $\vert$ Alexandre Klementiev
\end{quote}

\subsubsection*{ACL 2019: \href{./paper/P19-1659.pdf}{Multimodal Abstractive Summarization for How2 Videos}}

\begin{quote}
Author: Shruti Palaskar $\vert$ Jindřich Libovický $\vert$ Spandana Gella $\vert$ Florian Metze
\end{quote}

\subsubsection*{AAAI 2019: \href{./paper/4679-Article-Text-7718-1-10-20190707.pdf}{DeepChannel: Salience Estimation by Contrastive Learning for Extractive DocumentSummarization}}

\begin{quote}
Author: Jiaxin Shi; Chen Liang ; Lei Hou ; Juanzi Li;Hanwang Zhang;Zhiyuan Liu
\end{quote}

\subsubsection*{AAAI 2019: \href{./paper/4727-Article-Text-7766-2-10-20190721.pdf}{ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific PaperSummarization with Citation Networks}}

\begin{quote}
Author: Michihiro Yasunaga ; Jungo Kasai;Rui Zhang ;Alexander R Fabbri ;Irene Li ; Dan Friedman Dragomir Radev 
\end{quote}

\subsubsection*{AAAI 2019: \href{./paper/4724-Article-Text-7763-1-10-20190707.pdf}{Exploring Human Reading Cognition for Abstractive Text Summarization}}

\begin{quote}
Author: Min Yang ; Qiang Qu (SIAT); Zhou Zhao ;Xiaojun Chen ; Ying Shen; Wenting Tu 
\end{quote}

\subsubsection*{AAAI 2019: \href{./paper/4738-Article-Text-7777-1-10-20190707.pdf}{Generating Character Descriptions for Automatic Summarization of Fiction}}

\begin{quote}
Author: Weiwei Zhang; Jackie Chi Kit Cheung; Joel Oren 
\end{quote}

\subsubsection*{AAAI 2019: \href{./paper/4640-Article-Text-7679-1-10-20190707.pdf}{Towards Personalized Review Summarization via User-aware Sequence Network}}

\begin{quote}
Author: Junjie Li ; Haoran Li ; Chengqing Zong 
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/EMNLP2019_sharma_huang_hu_wang.pdf}{An Entity-Driven Framework for Abstractive Summarization}}

\begin{quote}
Author: Eva Sharma, Luyang Huang, Zhe Hu and Lu Wang
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1320.pdf}{Answers Unite! Unsupervised Metrics for Reinforced Summarization Models}}

\begin{quote}
Author: Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski and Jacopo Staiano
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1297.pdf}{Attribute-aware Sequence Network for Review Summarization}}

\begin{quote}
Author: Wenbo Wang, Yang Gao, Heyan Huang and Yuxiang Zhou
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1304.pdf}{Concept Pointer Network for Abstractive Summarization}}

\begin{quote}
Author: Eva Sharma, Luyang Huang, Zhe Hu and Lu Wang
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1301.pdf}{Contrastive Attention Mechanism for Abstractive Sentence Summarization}}

\begin{quote}
Author: Xiangyu Duan, Hongfei Yu, Mingming Yin, Min Zhang, Weihua Luo and Yue Zhang
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1298.pdf}{Extractive Summarization of Long Documents by Combining Global and Local Context}}

\begin{quote}
Author: Wen Xiao and Giuseppe Carenini
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/1909.08837.pdf}{How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing}}

\begin{quote}
Author: Shen Gao, Xiuying Chen, Piji Li, Zhangming Chan, Dongyan Zhao and Rui Yan
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1390.pdf}{Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator}}

\begin{quote}
Author: Xiaoyu Shen, Yang Zhao, Hui Su and Dietrich Klakow
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1302.pdf}{NCLS: Neural Cross-Lingual Summarization}}

\begin{quote}
Author: Junnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Jiajun Zhang, Shaonan Wang and Chengqing Zong
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1324.pdf}{Neural Extractive Text Summarization with Syntactic Compression}}

\begin{quote}
Author: Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong and Richard Socher
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1051.pdf}{Neural Text Summarization: A Critical Evaluation}}

\begin{quote}
Author: Eva Sharma, Luyang Huang, Zhe Hu and Lu Wang
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1300.pdf}{Reading Like HER: Human Reading Inspired Extractive Summarization}}

\begin{quote}
Author: Ling Luo, Xiang Ao, Yan Song, Feiyang Pan, Min Yang and Qing He
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D08-1080.pdf}{Subtopic-Driven Multi-Document Summarization}}

\begin{quote}
Author: Xin Zheng, Aixin Sun, Jing Li and Karthik Muthuswamy
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1386.pdf}{Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization}}

\begin{quote}
Author: Daniel Deutsch and Dan Roth
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1387.pdf}{Text Summarization with Pretrained Encoders}}

\begin{quote}
Author: Yang Liu and Mirella Lapata
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1389.pdf}{Unsupervised Sentence Summarization using the Information Bottleneck Principle}}

\begin{quote}
Author: Peter West, Ari Holtzman, Jan Buys and Yejin Choi
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1616.pdf}{Abstract Text Summarization: A Low Resource Challenge}}

\begin{quote}
Author: Shantipriya Parida and Petr Motlicek
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1117.pdf}{Attention Optimization for Abstractive Document Summarization}}

\begin{quote}
Author: Min Gui, Junfeng Tian, Rui Wang and Zhenglu Yang
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1620.pdf}{Countering the effects of lead bias in news summarization via multi-stage training and auxiliary losses}}

\begin{quote}
Author: Matt Grenander, Yue Dong, Jackie Chi Kit Cheung and Annie Louis
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1623.pdf}{Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization}}

\begin{quote}
Author: Siyao Li, Deren Lei, Pengda Qin and William Yang Wang
\end{quote}

\subsubsection*{EMNLP 2019: \href{./paper/D19-1116.pdf}{The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization}}

\begin{quote}
Author: Simeng Sun and Ani Nenkova
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1070.pdf}{Keyphrase Generation: A Text Summarization Struggle}}

\begin{quote}
Author: Erion Çano $\vert$ Ondřej Bojar
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1074.pdf}{Fast Concept Mention Grouping for Concept Map-based Multi-Document Summarization}}

\begin{quote}
Author: Tobias Falke $\vert$ Iryna Gurevych
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1173.pdf}{Single Document Summarization as Tree Induction}}

\begin{quote}
Author: Yang Liu $\vert$ Ivan Titov $\vert$ Mirella Lapata
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1204.pdf}{A Robust Abstractive System for Cross-Lingual Summarization}}

\begin{quote}
Author: Jessica Ouyang $\vert$ Boya Song $\vert$ Kathy McKeown
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1260.pdf}{Abstractive Summarization of Reddit Posts with Multi-level Memory Networks}}

\begin{quote}
Author: Byeongchang Kim $\vert$ Hyunwoo Kim $\vert$ Gunhee Kim
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1264.pdf}{Guiding Extractive Summarization with Question-Answering Rewards}}

\begin{quote}
Author: Kristjan Arumae $\vert$ Fei Liu
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-1395.pdf}{Question Answering as an Automatic Evaluation Metric for News Article Summarization}}

\begin{quote}
Author: Matan Eyal $\vert$ Tal Baumel $\vert$ Michael Elhadad
\end{quote}

\subsubsection*{NACCL 2019: \href{./paper/N19-4012.pdf}{LeafNATS: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization}}

\begin{quote}
Author: Tian Shi $\vert$ Ping Wang $\vert$ Chandan K. Reddy
\end{quote}
\end{quote}

\section*{2018}

\begin{quote}
\subsubsection*{ICLR 2018: \href{./paper/1705.04304.pdf}{A deep reinforced model for abstractive summarization}}

\begin{quote}
Paulus, Romain, Caiming Xiong, and Richard Socher  
\end{quote}

\subsubsection*{ACL 2018: \href{./paper/1805.11080.pdf}{Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting}}

\begin{quote}
Yen-Chun Chen, Mohit Bansal
\end{quote}

\subsubsection*{EMNLP 2018: \href{./paper/1808.07913.pdf}{Improving Abstraction in Text Summarization}}

\begin{quote}
Wojciech Kryściński, Romain Paulus, Caiming Xiong, Richard Socher
\end{quote}
\end{quote}

\end{document}
